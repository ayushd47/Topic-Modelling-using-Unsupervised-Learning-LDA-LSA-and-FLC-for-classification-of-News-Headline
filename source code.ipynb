{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78136af3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msb\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'abcnews-date-text.csv'\n",
    "raw_data = pd.read_csv(datafile, parse_dates=[0], infer_datetime_format=True)\n",
    "\n",
    "reindexed_data = raw_data['headline_text']\n",
    "reindexed_data.index = raw_data['publish_date']\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff46050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
    "    '''\n",
    "    returns a tuple of the top n words in a sample and their \n",
    "    accompanying counts, given a CountVectorizer object and text sample\n",
    "    '''\n",
    "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
    "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
    "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
    "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
    "    \n",
    "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
    "    for i in range(n_top_words):\n",
    "        word_vectors[i,word_indices[0,i]] = 1\n",
    "\n",
    "    words = [word[0].encode('ascii').decode('utf-8') for \n",
    "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
    "\n",
    "    return (words, word_values[0,:n_top_words].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "words, word_values = get_top_n_words(n_top_words=15,\n",
    "                                     count_vectorizer=count_vectorizer, \n",
    "                                     text_data=reindexed_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8), color = 'red')\n",
    "ax.bar(range(len(words)), word_values);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Top words in headlines dataset (excluding stop words)');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Number of occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ab386",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_headlines = [TextBlob(reindexed_data[i]).pos_tags for i in range(reindexed_data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_headlines_df = pd.DataFrame({'tags':tagged_headlines})\n",
    "\n",
    "word_counts = [] \n",
    "pos_counts = {}\n",
    "\n",
    "for headline in tagged_headlines_df[u'tags']:\n",
    "    word_counts.append(len(headline))\n",
    "    for tag in headline:\n",
    "        if tag[1] in pos_counts:\n",
    "            pos_counts[tag[1]] += 1\n",
    "        else:\n",
    "            pos_counts[tag[1]] = 1\n",
    "            \n",
    "print('Total number of words: ', np.sum(word_counts))\n",
    "print('Mean number of words per headline: ', np.mean(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ce6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stats.norm.pdf(np.linspace(0,14,50), np.mean(word_counts), np.std(word_counts))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "ax.hist(word_counts, bins=range(1,14), density=True);\n",
    "ax.plot(np.linspace(0,14,50), y, 'r--', linewidth=1);\n",
    "ax.set_title('Headline word lengths');\n",
    "ax.set_xticks(range(1,14));\n",
    "ax.set_xlabel('Number of words');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde532ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sorted_types = sorted(pos_counts, key=pos_counts.__getitem__, reverse=True)\n",
    "pos_sorted_counts = sorted(pos_counts.values(), reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "ax.bar(range(len(pos_counts)), pos_sorted_counts);\n",
    "ax.set_xticks(range(len(pos_counts)));\n",
    "ax.set_xticklabels(pos_sorted_types);\n",
    "ax.set_title('Part-of-Speech Tagging for Headlines Corpus');\n",
    "ax.set_xlabel('Type of Word');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f57fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_counts = reindexed_data.resample('M').count()\n",
    "yearly_counts = reindexed_data.resample('A').count()\n",
    "daily_counts = reindexed_data.resample('D').count()\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(18,16))\n",
    "ax[0].plot(daily_counts);\n",
    "ax[0].set_title('Daily Counts');\n",
    "ax[1].plot(monthly_counts);\n",
    "ax[1].set_title('Monthly Counts');\n",
    "ax[2].plot(yearly_counts);\n",
    "ax[2].set_title('Yearly Counts');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
    "small_text_sample = reindexed_data.sample(n=10000, random_state=0).values\n",
    "\n",
    "print('Headline before vectorization: {}'.format(small_text_sample[123]))\n",
    "\n",
    "small_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n",
    "\n",
    "print('Headline after vectorization: \\n{}'.format(small_document_term_matrix[123]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03899412",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ad9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ceeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_words = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lsa_categories, lsa_counts);\n",
    "ax.set_xticks(lsa_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_ylabel('Number of headlines');\n",
    "ax.set_title('LSA topic counts');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6872e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\n",
    "tsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_mean_topic_vectors(keys, two_dim_vectors):\n",
    "    '''\n",
    "    returns a list of centroid vectors from each predicted topic category\n",
    "    '''\n",
    "    mean_topic_vectors = []\n",
    "    for t in range(n_topics):\n",
    "        articles_in_that_topic = []\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == t:\n",
    "                articles_in_that_topic.append(two_dim_vectors[i])    \n",
    "        \n",
    "        articles_in_that_topic = np.vstack(articles_in_that_topic)\n",
    "        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n",
    "        mean_topic_vectors.append(mean_article_in_that_topic)\n",
    "    return mean_topic_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2451b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\n",
    "colormap = colormap[:n_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_words_lsa = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "lsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n",
    "\n",
    "plot = figure(title=\"t-SNE Clustering of {} LSA Topics\".format(n_topics), plot_width=700, plot_height=700)\n",
    "plot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n",
    "\n",
    "for t in range(n_topics):\n",
    "    label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n",
    "                  text=top_3_words_lsa[t], text_color=colormap[t])\n",
    "    plot.add_layout(label)\n",
    "    \n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
    "                                          random_state=0, verbose=0)\n",
    "lda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lda)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lda_categories, lda_counts);\n",
    "ax.set_xticks(lda_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_title('LDA topic counts');\n",
    "ax.set_ylabel('Number of headlines');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\n",
    "tsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "lda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n",
    "\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\n",
    "plot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n",
    "\n",
    "for t in range(n_topics):\n",
    "    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n",
    "                  text=top_3_words_lda[t], text_color=colormap[t])\n",
    "    plot.add_layout(label)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e361dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sample_size = 100000 \n",
    "\n",
    "big_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
    "big_text_sample = reindexed_data.sample(n=big_sample_size, random_state=0).values\n",
    "big_document_term_matrix = big_count_vectorizer.fit_transform(big_text_sample)\n",
    "\n",
    "big_lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online')\n",
    "big_lda_model.fit(big_document_term_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data = []\n",
    "for i in range(2003,2017+1):\n",
    "    yearly_data.append(reindexed_data['{}'.format(i)].values)\n",
    "\n",
    "yearly_topic_matrices = []\n",
    "for year in yearly_data:\n",
    "    document_term_matrix = big_count_vectorizer.transform(year)\n",
    "    topic_matrix = big_lda_model.transform(document_term_matrix)\n",
    "    yearly_topic_matrices.append(topic_matrix)\n",
    "\n",
    "yearly_keys = []\n",
    "for topic_matrix in yearly_topic_matrices:\n",
    "    yearly_keys.append(get_keys(topic_matrix))\n",
    "    \n",
    "yearly_counts = []\n",
    "for keys in yearly_keys:\n",
    "    categories, counts = keys_to_counts(keys)\n",
    "    yearly_counts.append(counts)\n",
    "\n",
    "yearly_topic_counts = pd.DataFrame(np.array(yearly_counts), index=range(2003,2017+1))\n",
    "yearly_topic_counts.columns = ['Topic {}'.format(i+1) for i in range(n_topics)]\n",
    "\n",
    "print(yearly_topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "sb.heatmap(yearly_topic_counts, cmap=\"YlGnBu\", ax=ax);\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
